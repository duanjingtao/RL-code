{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from GridWorld_v1 import GridWorld_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "desc = rows = 5  # 记得行数和列数这里要同步改\n",
    "columns = 5\n",
    "desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]\n",
    "\n",
    "gridworld = GridWorld_v1(forbidden_area_score=-1, score=1,desc=desc) \n",
    "print(gridworld.animator.fig.get_facecolor())\n",
    "gridworld.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(gamma: int, lmbda: int, td_delta: torch.Tensor):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0\n",
    "    for delta in td_delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    # print(f\"{advantage_list=}\")\n",
    "    return torch.tensor(advantage_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(torch.nn.Module):\n",
    "    \"\"\"只有一层隐藏层的Q网络\"\"\"\n",
    "\n",
    "    def __init__(self,state_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    \"\"\"只有一层隐藏层的Q网络\"\"\"\n",
    "\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(x,dim=1)  # 输出层不使用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,state_dim, hidden_dim, action_dim, actor_lr, critic_lr, \n",
    "                 lmbda, epochs,eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs\n",
    "        self.eps = eps\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict[\"states\"], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict[\"actions\"]).view(-1,1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict[\"rewards\"], dtype=torch.float).view(-1,1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict[\"next_states\"], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict[\"dones\"], dtype=torch.float).view(-1,1).to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1-dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        # td_delta = td_delta.detach()\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta)\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps, 1+self.eps) * advantage\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.actor.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 10000\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "lmbda = 0.99\n",
    "epochs = 10\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "env = gridworld\n",
    "state_dim = 2\n",
    "action_dim = 5\n",
    "\n",
    "agent = PPO(\n",
    "    state_dim,\n",
    "    hidden_dim,\n",
    "    action_dim,\n",
    "    actor_lr,\n",
    "    critic_lr,\n",
    "    lmbda,\n",
    "    epochs,\n",
    "    eps,\n",
    "    gamma,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/x5g6fdss0lx3bjtvj9_5cmwh0000gn/T/ipykernel_74052/1557795034.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.tensor(advantage_list, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, value: -17.0\n",
      "episode: 30, value: -19.0\n",
      "episode: 60, value: 0.0\n",
      "episode: 90, value: -14.0\n",
      "episode: 120, value: -1.0\n",
      "episode: 150, value: 1.0\n",
      "episode: 180, value: 1.0\n",
      "episode: 210, value: -3.0\n",
      "episode: 240, value: 1.0\n",
      "episode: 270, value: -9.0\n",
      "episode: 300, value: 0.0\n",
      "episode: 330, value: -13.0\n",
      "episode: 360, value: -3.0\n",
      "episode: 390, value: 1.0\n",
      "episode: 420, value: -2.0\n",
      "episode: 450, value: -1.0\n",
      "episode: 480, value: 0.0\n",
      "episode: 510, value: 0.0\n",
      "episode: 540, value: -1.0\n",
      "episode: 570, value: 1.0\n",
      "episode: 600, value: -3.0\n",
      "episode: 630, value: -2.0\n",
      "episode: 660, value: 0.0\n",
      "episode: 690, value: -1.0\n",
      "episode: 720, value: -11.0\n",
      "episode: 750, value: -3.0\n",
      "episode: 780, value: 0.0\n",
      "episode: 810, value: 0.0\n",
      "episode: 840, value: 0.0\n",
      "episode: 870, value: 0.0\n",
      "episode: 900, value: 0.0\n",
      "episode: 930, value: -1.0\n",
      "episode: 960, value: 0.0\n",
      "episode: 990, value: 0.0\n",
      "episode: 1020, value: 0.0\n",
      "episode: 1050, value: 1.0\n",
      "episode: 1080, value: 0.0\n",
      "episode: 1110, value: -8.0\n",
      "episode: 1140, value: 0.0\n",
      "episode: 1170, value: -1.0\n",
      "episode: 1200, value: 0.0\n",
      "episode: 1230, value: 0.0\n",
      "episode: 1260, value: 0.0\n",
      "episode: 1290, value: 0.0\n",
      "episode: 1320, value: 0.0\n",
      "episode: 1350, value: 0.0\n",
      "episode: 1380, value: 0.0\n",
      "episode: 1410, value: 0.0\n",
      "episode: 1440, value: 0.0\n",
      "episode: 1470, value: -1.0\n",
      "episode: 1500, value: 0.0\n",
      "episode: 1530, value: 0.0\n",
      "episode: 1560, value: 0.0\n",
      "episode: 1590, value: 0.0\n",
      "episode: 1620, value: 0.0\n",
      "episode: 1650, value: -1.0\n",
      "episode: 1680, value: 0.0\n",
      "episode: 1710, value: 0.0\n",
      "episode: 1740, value: 0.0\n",
      "episode: 1770, value: 0.0\n",
      "episode: 1800, value: 0.0\n",
      "episode: 1830, value: -1.0\n",
      "episode: 1860, value: 0.0\n",
      "episode: 1890, value: 0.0\n",
      "episode: 1920, value: 0.0\n",
      "episode: 1950, value: -2.0\n",
      "episode: 1980, value: 0.0\n",
      "episode: 2010, value: -2.0\n",
      "episode: 2040, value: -1.0\n",
      "episode: 2070, value: 0.0\n",
      "episode: 2100, value: 0.0\n",
      "episode: 2130, value: 0.0\n",
      "episode: 2160, value: 1.0\n",
      "episode: 2190, value: 0.0\n",
      "episode: 2220, value: 0.0\n",
      "episode: 2250, value: 0.0\n",
      "episode: 2280, value: -3.0\n",
      "episode: 2310, value: -1.0\n",
      "episode: 2340, value: 0.0\n",
      "episode: 2370, value: 0.0\n",
      "episode: 2400, value: 0.0\n",
      "episode: 2430, value: 0.0\n",
      "episode: 2460, value: 1.0\n",
      "episode: 2490, value: 0.0\n",
      "episode: 2520, value: 0.0\n",
      "episode: 2550, value: 0.0\n",
      "episode: 2580, value: -1.0\n",
      "episode: 2610, value: 0.0\n",
      "episode: 2640, value: 0.0\n",
      "episode: 2670, value: -24.0\n",
      "episode: 2700, value: 1.0\n",
      "episode: 2730, value: 0.0\n",
      "episode: 2760, value: 0.0\n",
      "episode: 2790, value: 0.0\n",
      "episode: 2820, value: 0.0\n",
      "episode: 2850, value: -1.0\n",
      "episode: 2880, value: 0.0\n",
      "episode: 2910, value: 0.0\n",
      "episode: 2940, value: 0.0\n",
      "episode: 2970, value: 0.0\n",
      "episode: 3000, value: 1.0\n",
      "episode: 3030, value: -1.0\n",
      "episode: 3060, value: -1.0\n",
      "episode: 3090, value: 0.0\n",
      "episode: 3120, value: 0.0\n",
      "episode: 3150, value: 0.0\n",
      "episode: 3180, value: -1.0\n",
      "episode: 3210, value: 0.0\n",
      "episode: 3240, value: -1.0\n",
      "episode: 3270, value: -2.0\n",
      "episode: 3300, value: 0.0\n",
      "episode: 3330, value: 0.0\n",
      "episode: 3360, value: -1.0\n",
      "episode: 3390, value: -9.0\n",
      "episode: 3420, value: 0.0\n",
      "episode: 3450, value: 0.0\n",
      "episode: 3480, value: 1.0\n",
      "episode: 3510, value: 0.0\n",
      "episode: 3540, value: 0.0\n",
      "episode: 3570, value: 0.0\n",
      "episode: 3600, value: 0.0\n",
      "episode: 3630, value: 0.0\n",
      "episode: 3660, value: 0.0\n",
      "episode: 3690, value: 0.0\n",
      "episode: 3720, value: 0.0\n",
      "episode: 3750, value: 0.0\n",
      "episode: 3780, value: 0.0\n",
      "episode: 3810, value: 0.0\n",
      "episode: 3840, value: 0.0\n",
      "episode: 3870, value: -1.0\n",
      "episode: 3900, value: 0.0\n",
      "episode: 3930, value: 0.0\n",
      "episode: 3960, value: 0.0\n",
      "episode: 3990, value: 0.0\n",
      "episode: 4020, value: 1.0\n",
      "episode: 4050, value: 0.0\n",
      "episode: 4080, value: 0.0\n",
      "episode: 4110, value: 0.0\n",
      "episode: 4140, value: 0.0\n",
      "episode: 4170, value: 1.0\n",
      "episode: 4200, value: -2.0\n",
      "episode: 4230, value: 0.0\n",
      "episode: 4260, value: -3.0\n",
      "episode: 4290, value: 0.0\n",
      "episode: 4320, value: 0.0\n",
      "episode: 4350, value: 0.0\n",
      "episode: 4380, value: 0.0\n",
      "episode: 4410, value: 0.0\n",
      "episode: 4440, value: 0.0\n",
      "episode: 4470, value: 0.0\n",
      "episode: 4500, value: -1.0\n",
      "episode: 4530, value: -3.0\n",
      "episode: 4560, value: 0.0\n",
      "episode: 4590, value: 0.0\n",
      "episode: 4620, value: 0.0\n",
      "episode: 4650, value: 0.0\n",
      "episode: 4680, value: 0.0\n",
      "episode: 4710, value: -3.0\n",
      "episode: 4740, value: 0.0\n",
      "episode: 4770, value: -2.0\n",
      "episode: 4800, value: 0.0\n",
      "episode: 4830, value: -1.0\n",
      "episode: 4860, value: 0.0\n",
      "episode: 4890, value: 0.0\n",
      "episode: 4920, value: 0.0\n",
      "episode: 4950, value: 0.0\n",
      "episode: 4980, value: -1.0\n",
      "episode: 5010, value: 0.0\n",
      "episode: 5040, value: 1.0\n",
      "episode: 5070, value: 0.0\n",
      "episode: 5100, value: 0.0\n",
      "episode: 5130, value: 0.0\n",
      "episode: 5160, value: 1.0\n",
      "episode: 5190, value: -3.0\n",
      "episode: 5220, value: 0.0\n",
      "episode: 5250, value: 0.0\n",
      "episode: 5280, value: 1.0\n",
      "episode: 5310, value: 0.0\n",
      "episode: 5340, value: 0.0\n",
      "episode: 5370, value: 1.0\n",
      "episode: 5400, value: 0.0\n",
      "episode: 5430, value: 0.0\n",
      "episode: 5460, value: 0.0\n",
      "episode: 5490, value: 0.0\n",
      "episode: 5520, value: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtake_action(state)\n\u001b[0;32m---> 19\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43mgridworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m transiton_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m     21\u001b[0m transiton_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/my_code/python_study/Code-of-RL-Beginning/codes/GridWorld_v1.py:228\u001b[0m, in \u001b[0;36mGridWorld_v1.step\u001b[0;34m(self, now_state, action)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_map[next_x][next_y]\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_map[next_x][next_y] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore:\n\u001b[1;32m    229\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (next_x, next_y), score, done\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_v = -100\n",
    "max_step = 2000\n",
    "for i in range(num_episodes):\n",
    "    state = gridworld.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    transiton_dict = {\n",
    "                \"states\": [],\n",
    "                \"actions\": [],\n",
    "                \"rewards\": [],\n",
    "                \"next_states\": [],\n",
    "                \"dones\": [],\n",
    "            }\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = agent.take_action(state)\n",
    "        next_state, reward, done = gridworld.step(state, action)\n",
    "        transiton_dict[\"states\"].append(state)\n",
    "        transiton_dict[\"actions\"].append(action)\n",
    "        transiton_dict[\"next_states\"].append(next_state)\n",
    "        transiton_dict[\"rewards\"].append(reward)\n",
    "        transiton_dict[\"dones\"].append(done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if step >= max_step:\n",
    "            done = True\n",
    "\n",
    "    agent.update(transiton_dict)\n",
    "    if episode_reward >= max_v:\n",
    "        agent.save(\"model.pth\")\n",
    "        max_v = episode_reward\n",
    "    if i % 30 == 0:\n",
    "        print(f\"episode: {i}, value: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 5])\n",
      "[[[1.55612270e-06 4.26340830e-06 9.99991059e-01 1.55612270e-06\n",
      "   1.55612270e-06]\n",
      "  [1.06396847e-07 1.06396847e-07 9.99999523e-01 1.06396847e-07\n",
      "   1.06396847e-07]\n",
      "  [1.45444923e-09 1.45444923e-09 1.00000000e+00 1.45444923e-09\n",
      "   1.45444923e-09]\n",
      "  [1.69530830e-11 1.69530830e-11 1.00000000e+00 1.65708641e-10\n",
      "   1.69530830e-11]\n",
      "  [2.03073140e-13 2.03073140e-13 1.00000000e+00 2.08695578e-11\n",
      "   2.03073140e-13]]\n",
      "\n",
      " [[6.83216203e-05 1.17663611e-02 9.88028705e-01 6.83216203e-05\n",
      "   6.83216203e-05]\n",
      "  [1.73325577e-06 1.73325577e-06 9.99992967e-01 1.73325577e-06\n",
      "   1.73325577e-06]\n",
      "  [4.49615563e-08 4.49615563e-08 9.99999642e-01 2.93574260e-07\n",
      "   4.49615563e-08]\n",
      "  [6.35189790e-10 6.35189790e-10 1.00000000e+00 5.37710250e-08\n",
      "   6.35189790e-10]\n",
      "  [7.91934747e-12 7.91934747e-12 1.00000000e+00 7.25099980e-09\n",
      "   7.91934747e-12]]\n",
      "\n",
      " [[9.41117585e-04 9.87051129e-01 1.18663665e-02 7.07574945e-05\n",
      "   7.07574945e-05]\n",
      "  [6.30711322e-04 4.08363715e-03 9.94799376e-01 2.43050046e-04\n",
      "   2.43050046e-04]\n",
      "  [3.66989912e-06 3.66989912e-06 9.99800861e-01 1.88099628e-04\n",
      "   3.66989912e-06]\n",
      "  [4.64680276e-08 4.64680276e-08 9.99955297e-01 4.46390331e-05\n",
      "   4.64680276e-08]\n",
      "  [6.76372847e-10 6.51257159e-10 9.99992609e-01 7.42774546e-06\n",
      "   6.51257159e-10]]\n",
      "\n",
      " [[4.08424850e-04 9.99583304e-01 3.91653930e-06 2.11067959e-06\n",
      "   2.11067959e-06]\n",
      "  [1.59507319e-02 9.81303573e-01 2.30117096e-03 2.22245028e-04\n",
      "   2.22245028e-04]\n",
      "  [2.28394512e-02 1.15552277e-03 9.40655768e-01 3.41937318e-02\n",
      "   1.15552277e-03]\n",
      "  [1.03859813e-04 1.06880343e-05 8.73937964e-01 1.25936687e-01\n",
      "   1.06880343e-05]\n",
      "  [7.16800798e-07 8.02016302e-08 9.87072527e-01 1.29265320e-02\n",
      "   8.02016302e-08]]\n",
      "\n",
      " [[1.35478229e-04 9.99864101e-01 6.26177865e-08 6.26177865e-08\n",
      "   6.26177865e-08]\n",
      "  [7.03138812e-03 9.92950201e-01 6.20295577e-06 6.20295577e-06\n",
      "   6.20295577e-06]\n",
      "  [9.53416884e-01 3.15827876e-02 3.31031578e-03 1.00167245e-02\n",
      "   1.67323952e-03]\n",
      "  [4.77112737e-03 3.04253335e-05 4.78670513e-03 9.90381300e-01\n",
      "   3.04253335e-05]\n",
      "  [5.55542974e-05 5.39983091e-07 1.62807535e-02 9.83662486e-01\n",
      "   5.39983091e-07]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/x5g6fdss0lx3bjtvj9_5cmwh0000gn/T/ipykernel_62802/3222823312.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_net.load_state_dict(torch.load(\"model.pth\"))\n",
      "/var/folders/j6/x5g6fdss0lx3bjtvj9_5cmwh0000gn/T/ipykernel_62802/3222823312.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  q_tables = np.array(q_tables).reshape(rows, columns, action_dim)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gridworld_idx = [(i, j) for i in range(rows) for j in range(columns)]\n",
    "gridworld_idx =  torch.tensor(gridworld_idx,dtype=torch.float32)\n",
    "best_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)  \n",
    "best_net.load_state_dict(torch.load(\"model.pth\"))\n",
    "with torch.no_grad():\n",
    "    q_tables =best_net(gridworld_idx)\n",
    "    print(q_tables.shape)\n",
    "    q_tables = np.array(q_tables).reshape(rows, columns, action_dim)\n",
    "# print(f\"q_tables_shape:{q_tables.shape}\")\n",
    "print(q_tables)\n",
    "policy = np.argmax(q_tables, axis=2)\n",
    "gridworld.show_policy(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
