{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from GridWorld_v1 import GridWorld_v1\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = rows = 5  # 记得行数和列数这里要同步改\n",
    "columns = 5\n",
    "gridworld = GridWorld_v1(forbidden_area_score=-1, score=1,desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]) \n",
    "gridworld.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(\n",
    "            *random.sample(self.buffer, batch_size)\n",
    "        )\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        # 目前buffer中的元素个数\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    \"\"\"只有一层隐藏层的Q网络\"\"\"\n",
    "\n",
    "    def __init__(self,state_dim,hidden_dim1,hidden_dim2,action_dim):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim1)\n",
    "        # self.fc2 = torch.nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim1, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        learning_rate,\n",
    "        gamma,\n",
    "        epsilons,\n",
    "        target_update,\n",
    "        device,\n",
    "    ) -> None:\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = Qnet(state_dim,64,32, action_dim).to(device)\n",
    "        self.target_net = Qnet(state_dim,64,32, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilons = epsilons\n",
    "        self.epsilon = epsilons[0]\n",
    "        self.target_update = target_update  # target net 更新频率\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "                q_value = self.q_net(state)\n",
    "                return torch.argmax(q_value).item()\n",
    "\n",
    "    def update(self, transiton_dict):\n",
    "        states = torch.tensor(transiton_dict[\"states\"], dtype=torch.float32).to(\n",
    "            self.device\n",
    "        )\n",
    "        actions = torch.tensor(transiton_dict[\"actions\"], dtype=torch.long)[:, None].to(\n",
    "            self.device\n",
    "        )\n",
    "        rewards = torch.tensor(transiton_dict[\"rewards\"], dtype=torch.float32)[\n",
    "            :, None\n",
    "        ].to(self.device)\n",
    "        next_states = torch.tensor(\n",
    "            transiton_dict[\"next_states\"], dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        dones = torch.tensor(transiton_dict[\"dones\"], dtype=torch.float32)[:, None].to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        q_values = self.q_net(states).gather(\n",
    "            1, actions\n",
    "        )  # 返回的是一个action_dim维的向量，取出对应的action value的值\n",
    "\n",
    "        max_next_q_values = self.target_net(next_states).max(dim=1)[0][:, None]\n",
    "        # max_next_q_values = self.q_net(next_states).max(dim=1)[0][:, None].detach()\n",
    "        # 只使用一个网络,不使用target_net\n",
    "\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "        # print(f\"{q_values.shape=}\")\n",
    "        # print(f\"{q_targets.shape=}\")\n",
    "        dqn_loss = self.loss(q_values, q_targets)\n",
    "        # print(\"loss:\", dqn_loss.item())\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count += 1\n",
    "    \n",
    "    def decay_epsilon(self,i):\n",
    "        self.epsilon = self.epsilons[i]\n",
    "        # epsilon = e^(-i/j)\n",
    "        # self.epsilon = math.exp(-i)\n",
    "        # self.epsilon = max(0.01, self.epsilon* self.epsilon_decay)\n",
    "        # print(f\"epsilon: {self.epsilon}\")\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.q_net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_episodes = 20000\n",
    "hidden_dim = 12\n",
    "gamma = 0.9\n",
    "# epsilon = 1\n",
    "target_update = 10\n",
    "buffer_size = 1000\n",
    "minimal_size = 100\n",
    "batch_size = 32\n",
    "epsilon_decay = 0.999\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "reply_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = 2\n",
    "action_dim = 5\n",
    "\n",
    "cutoff = 3000\n",
    "epsilon = np.exp(-np.arange(num_episodes) / (cutoff))\n",
    "\n",
    "epsilon[epsilon > epsilon[100 * int(num_episodes / cutoff)]] = epsilon[\n",
    "    100 * int(num_episodes / cutoff)\n",
    "]\n",
    "agent = DQN(\n",
    "    state_dim, action_dim, lr, gamma, epsilon, target_update, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_idx = [(i, j) for i in range(rows) for j in range(columns)]\n",
    "def policy_evaluation():\n",
    "\n",
    "    q_tables = (\n",
    "        agent.q_net(torch.tensor(gridworld_idx, dtype=torch.float32).to(device))\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "        .reshape(rows, columns, action_dim)\n",
    "    )\n",
    "    policy = np.argmax(q_tables, axis=2)\n",
    "    v = np.zeros((rows, columns))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        new_v = np.zeros((rows, columns))\n",
    "        max_diff = 0\n",
    "        cnt = 1\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                a = policy[i][j]\n",
    "                next_state, reward, done = gridworld.step([i, j], a)\n",
    "                new_v[i][j] = reward + agent.gamma * v[next_state[0], next_state[1]]\n",
    "                max_diff = max(max_diff, abs(new_v[i][j] - v[i][j]))\n",
    "        cnt += 1\n",
    "        v = new_v\n",
    "        if max_diff < 0.01:\n",
    "            # print(v.sum())\n",
    "            return v.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_v = -100\n",
    "for i in range(num_episodes):\n",
    "    state = gridworld.reset((0,2))\n",
    "    done = False\n",
    "    now_reward = 0\n",
    "    while not done:\n",
    "        action = agent.take_action(state)\n",
    "        next_state, reward, done = gridworld.step(state, action)\n",
    "        reply_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        now_reward += reward\n",
    "\n",
    "        if len(reply_buffer) > minimal_size:\n",
    "            b_s, b_a, b_r, b_ns, b_d = reply_buffer.sample(batch_size)\n",
    "            transiton_dict = {\n",
    "                \"states\": b_s,\n",
    "                \"actions\": b_a,\n",
    "                \"rewards\": b_r,\n",
    "                \"next_states\": b_ns,\n",
    "                \"dones\": b_d,\n",
    "            }\n",
    "            agent.update(transiton_dict)\n",
    "    agent.decay_epsilon(i)\n",
    "    # print(f\"episode: {i}, reward: {now_reward}\")\n",
    "    if now_reward >= max_v:\n",
    "        agent.save(\"model.pth\")\n",
    "        max_v = now_reward\n",
    "    if i % 30 == 0:\n",
    "        print(f\"episode: {i}, value: {now_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_idx = [(i, j) for i in range(rows) for j in range(columns)]\n",
    "best_net = Qnet(state_dim,64,32, action_dim).to(device)\n",
    "best_net.load_state_dict(torch.load(\"model.pth\"))\n",
    "q_tables = (\n",
    "    best_net(torch.tensor(gridworld_idx, dtype=torch.float32).to(device))\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    "    .reshape(rows, columns, action_dim)\n",
    ")\n",
    "# print(f\"q_tables_shape:{q_tables.shape}\")\n",
    "print(q_tables)\n",
    "policy = np.argmax(q_tables, axis=2)\n",
    "gridworld.show_policy(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
